{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f717e22-8e53-4bf0-a550-4659fc27d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.layers import LSTM,Input,Conv1D, SpatialDropout1D,Dense, BatchNormalization, Activation, MaxPooling1D, GlobalAveragePooling1D, Add,Flatten,Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d5d6e-c6ce-4d72-b156-bbe4f52a734c",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c72e285-5758-47e9-91cd-3b6e50ad20ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 512) (1600000,) (498, 512) (498,)\n"
     ]
    }
   ],
   "source": [
    "with open('Train_X.npy', 'rb') as f:\n",
    "    Train_X = np.load(f,allow_pickle=True)\n",
    "\n",
    "with open('Train_y.npy', 'rb') as f:\n",
    "    Train_y = np.load(f,allow_pickle=True)\n",
    "\n",
    "with open('Test_X.npy', 'rb') as f:\n",
    "    Test_X = np.load(f,allow_pickle=True)\n",
    "    \n",
    "with open('Test_y.npy', 'rb') as f:\n",
    "    Test_y = np.load(f,allow_pickle=True)\n",
    "\n",
    "print(Train_X.shape,Train_y.shape,Test_X.shape,Test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7072d07a-ded0-4429-86f1-cb163c1cd901",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_y[Train_y == 4] = 1\n",
    "Test_y[Test_y == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a7928-92e8-428e-bb3c-752bb8add18a",
   "metadata": {},
   "source": [
    "### Logsitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28bf422-f539-4e36-a56b-e630d0736d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5180722891566265"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0,solver='saga').fit(Train_X, Train_y)\n",
    "clf.score(Test_X, Test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54ad8c-b78a-4012-95af-b3a85039570f",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f76835-343b-4909-a887-087d7de18b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5160642570281124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=11)\n",
    "neigh.fit(Train_X, Train_y)\n",
    "neigh.score(Test_X, Test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40074a-bcb4-4060-a711-99f191fb55b4",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c960c4-5961-4791-be7f-fbdb99532e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 508, 32)           192       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 254, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 250, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 125, 32)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 5,617\n",
      "Trainable params: 5,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x163777940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 21:53:02.186610: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-12 21:53:02.186881: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x163777940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x163777940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12498/12500 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.5300WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17ab8b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17ab8b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x17ab8b4c0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "12500/12500 [==============================] - 321s 26ms/step - loss: 0.6903 - accuracy: 0.5300 - val_loss: 0.6319 - val_accuracy: 0.3795\n",
      "Epoch 2/10\n",
      "12500/12500 [==============================] - 317s 25ms/step - loss: 0.6891 - accuracy: 0.5362 - val_loss: 0.6694 - val_accuracy: 0.3996\n",
      "Epoch 3/10\n",
      "12500/12500 [==============================] - 320s 26ms/step - loss: 0.6885 - accuracy: 0.5385 - val_loss: 0.6682 - val_accuracy: 0.3855\n",
      "Epoch 4/10\n",
      "12500/12500 [==============================] - 309s 25ms/step - loss: 0.6882 - accuracy: 0.5392 - val_loss: 0.6513 - val_accuracy: 0.3735\n",
      "Epoch 5/10\n",
      "12500/12500 [==============================] - 300s 24ms/step - loss: 0.6880 - accuracy: 0.5396 - val_loss: 0.6653 - val_accuracy: 0.3715\n",
      "Epoch 6/10\n",
      "12500/12500 [==============================] - 307s 25ms/step - loss: 0.6878 - accuracy: 0.5404 - val_loss: 0.6374 - val_accuracy: 0.3916\n",
      "Epoch 7/10\n",
      "12500/12500 [==============================] - 311s 25ms/step - loss: 0.6877 - accuracy: 0.5405 - val_loss: 0.6124 - val_accuracy: 0.3855\n",
      "Epoch 8/10\n",
      "12500/12500 [==============================] - 345s 28ms/step - loss: 0.6875 - accuracy: 0.5406 - val_loss: 0.6355 - val_accuracy: 0.3855\n",
      "Epoch 9/10\n",
      "12500/12500 [==============================] - 320s 26ms/step - loss: 0.6873 - accuracy: 0.5420 - val_loss: 0.6522 - val_accuracy: 0.3876\n",
      "Epoch 10/10\n",
      "12500/12500 [==============================] - 321s 26ms/step - loss: 0.6872 - accuracy: 0.5422 - val_loss: 0.6348 - val_accuracy: 0.3855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x168ebc700>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input((512,1)))\n",
    "\n",
    "    model.add(Conv1D(32,kernel_size=5,activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "    model.add(Conv1D(32,kernel_size=5,activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "   \n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def step_decay(epoch):\n",
    "    \"\"\"\n",
    "    a function to decay the learning rate 0.94 every 2 epoch\n",
    "    \"\"\"\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.95\n",
    "    epochs_drop = 2\n",
    "    end_lr = 0.00001\n",
    "    lrate = initial_lrate * np.power(drop,  \n",
    "        np.floor((1+epoch)/epochs_drop))\n",
    "    if lrate > end_lr:\n",
    "        return lrate\n",
    "    else:\n",
    "        return end_lr\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "el = EarlyStopping(monitor='val_loss', patience=8)\n",
    "\n",
    "#parameter setting\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "optimizer = Adam(0.01)\n",
    "\n",
    "#model compile\n",
    "model = get_model()\n",
    "model.compile(optimizer,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(np.expand_dims(Train_X,axis=2),Train_y,epochs = epochs, \n",
    "                                      batch_size = batch_size,validation_data=(np.expand_dims(Test_X,axis=2),Test_y),\n",
    "                                      callbacks=[lr_scheduler,el],verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e95c4-b08b-4c1e-ad18-4a92cc8bbdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
